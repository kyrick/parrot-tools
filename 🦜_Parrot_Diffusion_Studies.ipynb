{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNLPem2RaVKJ",
    "tags": []
   },
   "source": [
    "# Parrot Diffusion Studies ðŸ¦œðŸŽ¨\n",
    "\n",
    "This notebook is designed to study parameters, settings and prompts against the [Stable Diffusion model](https://github.com/CompVis/stable-diffusion).\n",
    "\n",
    "This notebook maintained by [Stephen Young](https://twitter.com/KyrickYoung) or SteveTheNinja#0616\n",
    "\n",
    "Visit the Parrot Zone [parrotzone.art](http://www.parrotzone.art)\n",
    "\n",
    "## Changelog\n",
    "### 2.0\n",
    "- paperspace port!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write output images to folder /root/dev/parrot-tools/images_out\n"
     ]
    }
   ],
   "source": [
    "# Create images out\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    google_drive = True\n",
    "except:\n",
    "    google_drive = False\n",
    "\n",
    "if google_drive:\n",
    "    ROOT_FOLDER = \"generative\"\n",
    "    PROJECT_FOLDER = \"parrot_studies\"\n",
    "    MOUNTED_PATH = Path('/content/drive')\n",
    "    MYDRIVE_PATH = MOUNTED_PATH / \"MyDrive\"\n",
    "    PROJECT_PATH = MYDRIVE_PATH /  ROOT_FOLDER / PROJECT_FOLDER\n",
    "\n",
    "    IMAGES_OUT = PROJECT_PATH/ \"images_out\"\n",
    "\n",
    "    drive.mount(str(MOUNTED_PATH))\n",
    "\n",
    "    os.makedirs(PROJECT_PATH, exist_ok=True)\n",
    "    os.makedirs(IMAGES_OUT, exist_ok=True)\n",
    "else:\n",
    "    # setup paths\n",
    "    # REPLACE THIS WITH YOUR PATH\n",
    "    # HINT: type $PWD in a terminal\n",
    "    NOTEBOOK_PATH = Path(\"/root/dev/parrot-tools\")\n",
    "    IMAGES_OUT = NOTEBOOK_PATH / \"images_out\"\n",
    "\n",
    "    os.makedirs(IMAGES_OUT, exist_ok=True)\n",
    "\n",
    "print(\"Will write output images to folder\", IMAGES_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "OFkTxsKybPYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydantic==1.7.4 in /root/.pyenv/versions/3.10.12/envs/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages (1.7.4)\n",
      "Requirement already satisfied: pillow in /root/.pyenv/versions/3.10.12/envs/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages (10.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@title Setup\n",
    "#  scipy ftfy \n",
    "%pip install transformers accelerate safetensors omegaconf diffusers[torch]==0.19.0 invisible-watermark>=0.2.0\n",
    "%pip install pydantic==1.7.4 pillow\n",
    "# %pip install git+https://github.com/kyrick/parrot-tools.git --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "WVlISo2FffMA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from parrot_tools.utils import prepare_prompts_for_study\n",
    "from parrot_tools.generate import run_prompts, BatchSettings\n",
    "\n",
    "# THIS IS A DIRTY HACK TO SILENCE THE PROGRESS BAR\n",
    "# THE TQDM PROGRESS BAR BOTTLENECKS CELL OUTPUT AND SLOWS THE NOTEBOOK\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partialmethod\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "PzeQ2Qucd3Io"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableDiffusionXLPipeline {\n",
       "  \"_class_name\": \"StableDiffusionXLPipeline\",\n",
       "  \"_diffusers_version\": \"0.19.0\",\n",
       "  \"force_zeros_for_empty_prompt\": true,\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Model\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "# Instructions:\n",
    "# 1. Download the safetensors file from hugging face. \n",
    "# 2. Upload it to a folder for example a drive location. \n",
    "# 3. Get the absolute path to the file and past it here.\n",
    "# 4. ???\n",
    "# 5. Profit.\n",
    "path_to_model = \"/root/dev/parrot-tools/model/sd_xl_base_1.0_with_09_vae.safetensors\"\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    path_to_model, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M5ImX6hPrFhv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a portrait of a character in a scenic environment by Gediminas Pranckevicius - Gediminas_Pranckevicius\n",
      "a building in a stunning landscape by Gediminas Pranckevicius - Gediminas_Pranckevicius\n",
      "a portrait of a character in a scenic environment by Ivan Aivazovsky - Ivan_Aivazovsky\n",
      "a building in a stunning landscape by Ivan Aivazovsky - Ivan_Aivazovsky\n",
      "a portrait of a character in a scenic environment by RHADS - RHADS\n",
      "a building in a stunning landscape by RHADS - RHADS\n",
      "a portrait of a character in a scenic environment, graphic novel - graphic_novel\n",
      "a building in a stunning landscape, graphic novel - graphic_novel\n",
      "a portrait of a character in a scenic environment, vaporwave - vaporwave\n",
      "a building in a stunning landscape, vaporwave - vaporwave\n"
     ]
    }
   ],
   "source": [
    "base_prompts = \"\"\"\n",
    "a portrait of a character in a scenic environment\n",
    "a building in a stunning landscape\n",
    "\"\"\"\n",
    "\n",
    "# Supported formats:\n",
    "# Gediminas Pranckevicius - first, last\n",
    "# Aivazovsky, Ivan        - last, first (supports tabs if pasted from google sheets!)\n",
    "# de Kooning, Willem      - 2+ words in the last name\n",
    "# Death Burger            - a nickname of two or more words\n",
    "# RHADS                   - a single nickname\n",
    "styles_to_study = \"\"\"\n",
    "Pranckevicius, Gediminas\n",
    "Aivazovsky\tIvan\n",
    "N/A, RHADS\n",
    "\"\"\"\n",
    "modifiers_to_study = \"\"\"\n",
    "graphic novel\n",
    "vaporwave\n",
    "\"\"\"\n",
    "\n",
    "# This text will be added to the end of every prompt\n",
    "append_to_all_prompts = \"\"\n",
    "\n",
    "prompts_to_run = prepare_prompts_for_study(base_prompts, styles_to_study, modifiers_to_study, append_to_all_prompts)\n",
    "\n",
    "# print out the prompts to sanity check\n",
    "for p in prompts_to_run:\n",
    "    print(p.prompt, \"-\", p.base_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "7LTg896csdHx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:13<00:00,  1.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:11<00:00,  1.30s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:11<00:00,  1.30s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:11<00:00,  1.27s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m\n\u001b[1;32m     22\u001b[0m display_grid_images \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m batch_settings \u001b[39m=\u001b[39m BatchSettings(\n\u001b[1;32m     25\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     26\u001b[0m     batch_name\u001b[39m=\u001b[39mbatch_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     display_grid_images\u001b[39m=\u001b[39mdisplay_grid_images,\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m run_prompts(pipe, prompts_to_run, batch_settings)\n",
      "File \u001b[0;32m~/dev/parrot-tools/parrot_tools/generate/stable_diffusion.py:62\u001b[0m, in \u001b[0;36mrun_prompts\u001b[0;34m(pipe, prompts, batch_settings)\u001b[0m\n\u001b[1;32m     60\u001b[0m images \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m trange(settings\u001b[39m.\u001b[39mbatch\u001b[39m.\u001b[39mbatch_size):\n\u001b[0;32m---> 62\u001b[0m     res \u001b[39m=\u001b[39m generate_images(\n\u001b[1;32m     63\u001b[0m         pipe,\n\u001b[1;32m     64\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt\u001b[39m.\u001b[39;49mprompt,\n\u001b[1;32m     65\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     66\u001b[0m         steps\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mbatch\u001b[39m.\u001b[39;49msteps,\n\u001b[1;32m     67\u001b[0m         cfg_scale\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mbatch\u001b[39m.\u001b[39;49mcfg_scale,\n\u001b[1;32m     68\u001b[0m         height\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mbatch\u001b[39m.\u001b[39;49mimage_h,\n\u001b[1;32m     69\u001b[0m         width\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mbatch\u001b[39m.\u001b[39;49mimage_w,\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     res_images \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mimages\n\u001b[1;32m     72\u001b[0m     images \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res_images\n",
      "File \u001b[0;32m~/dev/parrot-tools/parrot_tools/generate/stable_diffusion.py:29\u001b[0m, in \u001b[0;36mgenerate_images\u001b[0;34m(pipe, prompt, seed, steps, cfg_scale, height, width)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_images\u001b[39m(\n\u001b[1;32m     18\u001b[0m     pipe,\n\u001b[1;32m     19\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     width: \u001b[39mint\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StableDiffusionPipelineOutput:\n\u001b[1;32m     27\u001b[0m     generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m---> 29\u001b[0m     res \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     30\u001b[0m         prompt,\n\u001b[1;32m     31\u001b[0m         num_inference_steps\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m     32\u001b[0m         guidance_scale\u001b[39m=\u001b[39;49mcfg_scale,\n\u001b[1;32m     33\u001b[0m         generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m     34\u001b[0m         height\u001b[39m=\u001b[39;49mheight,\n\u001b[1;32m     35\u001b[0m         width\u001b[39m=\u001b[39;49mwidth,\n\u001b[1;32m     36\u001b[0m         num_images_per_prompt\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:804\u001b[0m, in \u001b[0;36mStableDiffusionXLPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[1;32m    803\u001b[0m added_cond_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtext_embeds\u001b[39m\u001b[39m\"\u001b[39m: add_text_embeds, \u001b[39m\"\u001b[39m\u001b[39mtime_ids\u001b[39m\u001b[39m\"\u001b[39m: add_time_ids}\n\u001b[0;32m--> 804\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(\n\u001b[1;32m    805\u001b[0m     latent_model_input,\n\u001b[1;32m    806\u001b[0m     t,\n\u001b[1;32m    807\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m    808\u001b[0m     cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    809\u001b[0m     added_cond_kwargs\u001b[39m=\u001b[39;49madded_cond_kwargs,\n\u001b[1;32m    810\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    811\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    813\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:945\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[39m# 4. mid\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmid_block \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmid_block(\n\u001b[1;32m    946\u001b[0m         sample,\n\u001b[1;32m    947\u001b[0m         emb,\n\u001b[1;32m    948\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    949\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    950\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    951\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    952\u001b[0m     )\n\u001b[1;32m    954\u001b[0m \u001b[39mif\u001b[39;00m is_controlnet:\n\u001b[1;32m    955\u001b[0m     sample \u001b[39m=\u001b[39m sample \u001b[39m+\u001b[39m mid_block_additional_residual\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:614\u001b[0m, in \u001b[0;36mUNetMidBlock2DCrossAttn.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    612\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets[\u001b[39m0\u001b[39m](hidden_states, temb)\n\u001b[1;32m    613\u001b[0m \u001b[39mfor\u001b[39;00m attn, resnet \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets[\u001b[39m1\u001b[39m:]):\n\u001b[0;32m--> 614\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn(\n\u001b[1;32m    615\u001b[0m         hidden_states,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    618\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    619\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    620\u001b[0m         return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    621\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m    624\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/transformer_2d.py:292\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 292\u001b[0m     hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    293\u001b[0m         hidden_states,\n\u001b[1;32m    294\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    296\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    297\u001b[0m         timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    298\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    299\u001b[0m         class_labels\u001b[39m=\u001b[39;49mclass_labels,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/attention.py:155\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    151\u001b[0m     norm_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(hidden_states)\n\u001b[1;32m    153\u001b[0m cross_attention_kwargs \u001b[39m=\u001b[39m cross_attention_kwargs \u001b[39mif\u001b[39;00m cross_attention_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 155\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn1(\n\u001b[1;32m    156\u001b[0m     norm_hidden_states,\n\u001b[1;32m    157\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monly_cross_attention \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    158\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ada_layer_norm_zero:\n\u001b[1;32m    162\u001b[0m     attn_output \u001b[39m=\u001b[39m gate_msa\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m attn_output\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/attention_processor.py:322\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    319\u001b[0m     \u001b[39m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessor(\n\u001b[1;32m    323\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    324\u001b[0m         hidden_states,\n\u001b[1;32m    325\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    326\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    327\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/parrot-tools/envs/parrot-tools-3/lib/python3.10/site-packages/diffusers/models/attention_processor.py:1095\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb)\u001b[0m\n\u001b[1;32m   1092\u001b[0m inner_dim \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1093\u001b[0m head_dim \u001b[39m=\u001b[39m inner_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m attn\u001b[39m.\u001b[39mheads\n\u001b[0;32m-> 1095\u001b[0m query \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, attn\u001b[39m.\u001b[39;49mheads, head_dim)\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m   1097\u001b[0m key \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attn\u001b[39m.\u001b[39mheads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m   1098\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, attn\u001b[39m.\u001b[39mheads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Do The Run\n",
    "#@markdown ## Run Settings\n",
    "#@markdown set seed to -1 for random seed\n",
    "batch_name = \"StableXLStudy\"\n",
    "#@markdown batch_size is images per prompt\n",
    "manual_seed = -1\n",
    "steps = 20\n",
    "cfg_scale = 7.0\n",
    "\n",
    "#@markdown ##Image Settings\n",
    "image_ext = \"jpeg\"\n",
    "width = 256\n",
    "height = 256\n",
    "\n",
    "#@markdown ##Grid Settings\n",
    "batch_size = 9\n",
    "grid_cols = 3\n",
    "grid_padding = 0\n",
    "#@markdown color name (`black`, `white`, `pink`, etc.) or hex code in format `#00004c'`\n",
    "grid_bg_color = \"white\"\n",
    "save_individual_images = False\n",
    "display_grid_images = True\n",
    "\n",
    "batch_settings = BatchSettings(\n",
    "    batch_size=batch_size,\n",
    "    batch_name=batch_name,\n",
    "    base_path=IMAGES_OUT,\n",
    "    steps=steps,\n",
    "    cfg_scale=cfg_scale,\n",
    "    seed=manual_seed,\n",
    "    image_ext=image_ext,\n",
    "    image_w=width,\n",
    "    image_h=height,\n",
    "    grid_cols=grid_cols,\n",
    "    grid_padding=grid_padding,\n",
    "    grid_bg_color=grid_bg_color,\n",
    "    save_individual_images=save_individual_images,\n",
    "    display_grid_images=display_grid_images,\n",
    ")\n",
    "\n",
    "run_prompts(pipe, prompts_to_run, batch_settings)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3.9.9 ('parrot-tools-Fxf-GLwc-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38b47f231eab3b2322e28e40859caed59aa62c188843bf2bdb808ee5c2afa12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
